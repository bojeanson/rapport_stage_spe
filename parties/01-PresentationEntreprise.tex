L’évolution des technologies et leurs usages ont fait exploser la quantité de données générées. Selon IBM, 2.5 milliard de gigabytes (GB) de données a été générée tout les jours de l'année 2012. De plus, cette quantité de données, double tous les deux ans. Cependant, seules 0,05\% de ces données sont analysées.\\

L’exploration ou la fouille de données (\og data mining \fg) consiste à en extraire des informations utiles, et ceci peut s’avérer très fructueux. La question principale qui se pose est de savoir comment utiliser intelligemment cette immense masse de données pour en tirer une plus-value ?\\
C’est le rôle des entreprises spécialisées dans l'exploitation de ces données.

\section{L'entreprise}
    La société Data Publica a été fondée en juillet 2011 par François Bancilhon et Christian Frisch, respectivement l'actuel directeur général et l'actuel directeur technique.

    \subsection{L'activité de Data Publica}
        Data Publica est un des précurseurs de l'open data en France. Cette société , qui a bénéficié d’investissements technologiques faits en 2010 dans le cadre d’un projet de R\&D, a été financée initialement par un groupe de business angels et le fonds d’amorçage \textbf{IT Translation}.
        Data Publica est une start-up spécialisée dans les données entreprises, l'open data, le big data et la dataviz. C'est une société relativement jeune, axée R\&D. Son leitmotiv, alimenté par une équipe très dynamique et compétente, est la recherche constante du dépassement technique.

        \paragraph{}
            Historiquement, Data Publica ne faisait que de l'open-data. C'est-à-dire que la société se servait de données accessibles à tous (provenant d'institutions gouvernementales notamment) pour créer des jeux de données sur mesure pour des entreprises. Ainsi, la société s'est spécialisé dans l'identification des sources de données, leur extraction et leur transformation en données structurées.

        \paragraph{}
            Depuis quelques années, Data Publica se spécialise dans les données sur les entreprises françaises en dépit de son activité open-data qu'elle a progressivement mis de côté. Les services qu'elle propose ne sont plus tout-à-fait les mêmes. En effet, Data Publica réutilise les données open-data concernant les entreprises française dans son produit phare. Ce produit est lui même conçu pour les entreprises du B2B. Le produit est décrit en partie \ref{c_radar}.

        \paragraph{}
            Data Publica participe également à de nombreux projets de recherche français et européens tels que XDATA, Diachron ou Poqemon, en partenariat avec l'INRIA.

    \subsection{L'équipe de Data Publica}
        Data Publica emploie quatorze personnes réparties en deux équipes : une équipe commerciale (quatre personnes) et une équipe technique (dix développeurs). Les deux équipes travaillent chacune dans son open-space. Pendant mon stage, j'ai été immergé au sein de l'équipe technique.

        \paragraph{L'équipe technique :}
            Elle est composée de dix développeurs (ordonnés par ancienneté visible en annexe \ref{annexe:teamd_data_publica}) :
            \begin{itemize}
                \item Christian Frisch, directeur de l'équipe
                \item Thomas Dudouet, développeur Back-end Java
                \item Guillaume Lebourgeois, chef de produit C-Radar
                \item Samuel Charron, Data scientist Python et mon maître de stage
                \item Loïc Petit, développeur Back-end Java (JBM)
                \item Clément Chastagnol, Data scientist Python et mon maître de stage
                \item Clément Déon, développeur Front-end
                \item Fabien Bréant, développeur Back-end
                \item Jacques Belissent, développeur Back-end
                \item Vincent Ysmal, développeur Back-end Java
            \end{itemize}

        \paragraph{L'équipe commerciale :}
            Elle est composée de cinq commerciaux (ordonnés par ancienneté visible en annexe \ref{annexe:teamc_data_publica}) :
            \begin{itemize}
                \item François Bancilhon, directeur général
                \item Benjamin Gans, Responsable Communication et Marketing
                \item Emmanuel Jouanne, Business Development Manager
                \item Philippe Spenato, Ingénieur d'affaire
                \item Justine Pourrat, Responsable Communication et marketing
            \end{itemize}

\section{C-Radar}\label{c_radar}
    \subsection{Présentation commerciale de C-Radar}
        Son produit est un moteur de recherche B2B (Business to Business). Celui-ci a pour objectif de permettre aux services ventes et marketing des entreprises B2B de vendre plus et mieux.\\
        Ce moteur de recherche, appelé C-Radar, est un produit de vente prédictive construit sur une base de référence des entreprises françaises. Il regroupe beaucoup d'informations sur les entreprises françaises, dont notamment leurs informations administratives, financières et toutes celles qui découlent de leur communication sur les réseaux sociaux et le web.\\

        C-Radar est un concentré de technologies du big data. En effet, il utilise diverses technologies comme le crawling, le scraping ou encore le machine learning. Ceci afin d'offrir à l'utilisateur diverses fonctionnalités : moteur de recherche d'entreprises, fiche d'activité d'entreprises avec contacts commerciaux, détection de nouveaux prospects, scoring de prospects existants, segmentation automatique d'entreprises, identification de marché, etc.

    \subsection{Présentation technique de C-Radar}
        \subsubsection{Les technologies utilisées par C-Radar}
            Pour répondre aux problématiques auxquelles Data Publica se confronte, la société a acquis 4 expertises majeures :
            \begin{itemize}
                \item Le web crawling / web scraping : la récupération des données ;
                \item Le data mining / text mining : l'analyse, l’extraction et l’enrichissement des données;
                \item Le machine learning : l'apprentissage automatique à partir de données structurées ;
                \item La dataviz : la visualisation des données.
            \end{itemize}

            \paragraph{Le crawling :}
                Le crawling est l’action réalisée par un programme informatique, appelé le web crawler, qui va de site en site afin d’en extraire automatiquement toute l’information qui est présente sur les différentes pages. Cette technique est utilisée notamment pour l’extraction de données non structurées: la structure du site n’est pas connue à l’avance, l’extraction des données se fait directement sur le contenu (c’est à dire le code HTML) de la page crawlée. Ce processus est \og brutal \fg.

            \paragraph{Le scraping :}
                Le scraping est l’action réalisée par un programme informatique pour extraire des unités d’information structurées d’un site web. Contrairement au crawling, il est question d’extraire des données précises, et pas la totalité des données disponibles sur le site. Le site \og scrappé \fg et sa structure doivent donc être connus et analysés à l’avance afin d’adapter le scraper au site. Ce processus est \og intelligent \fg.

            \paragraph{Le data mining / text mining :}
                Une fois des sites web crawlés et scrapés, ou que des flux (RSS ou réseaux sociaux) ont été captés, on peut commencer à analyser le contenu récupéré à la recherche d'informations sous forme de motifs particuliers. On analyse afin de normaliser les encodages, et on extrait des dates, des numéros de téléphone, etc. Ces normalisations et extractions sont effectuées sous forme de liste.\\
                Principalement, cette phase consiste à regarder les données \og dans le fond des yeux \fg \footnote{Expression empruntée à \href{http://asi.insa-rouen.fr/enseignants/~scanu/}{Stéphane Canu}.} afin de voir leur fond mais aussi leur forme.

            \paragraph{Le machine learning :}
                L'idée du machine learning est de pouvoir extraire automatiquement des informations d'une nouvelle donnée et ainsi prédire une classe de donnée.\\
                Quand les données sont correctement formatées et normalisées, on peut construire des applications capables d'apprendre automatiquement de ces données, de les classifier. Ainsi quand on aura une nouvelle donnée l'application sera capable de prédire sa classe d'après ses caractéristiques.\\
                Exemple (tiré de Wikipédia\autocite{wiki_ml}) : La reconnaissance de caractères manuscrits est une tâche complexe, car deux caractères similaires ne sont jamais exactement égaux. On peut concevoir un système d'apprentissage automatique qui apprend à reconnaître des caractères en observant des \og exemples \fg, c'est-à-dire des caractères connus.

            \paragraph{La dataviz :}
                C'est la dernière étape. Elle présente les données de manière visuelle et interprétable. Ainsi, on peut comprendre plus facilement et rapidement les informations extraites des données. Un exemple de dataviz est présenté en annexe \ref{annexe:firmo}.

        \subsubsection{Construction des fiches entreprises dans C-Radar}
            Pour construire les fiches entreprises, on part d'un fichier de SIREN\footnote{Système informatique du répertoire des entreprises. C'est un code Insee unique servant à identifier une entreprise française.} contenant les données administratives de l'entreprise (nom, SIREN, adresse, ...). Ensuite, on va essayer d'associer au numéro SIREN de l'entreprise un site web. Une fois le site web détecté, on va le crawler et en extraire de l'information structurée. Enfin, on enrichie encore les données de l'entreprise, en cherchant de l'information sur des sites web externes (tels que \href{https://www.linkedin.com/}{linkedin} et \href{fr.viadeo.com/}{viadeo}). Il ne reste plus qu'à agréger toutes ces données pour former une fiche d'entreprise complète.

        \subsubsection{L'architecture technique de C-Radar}
        \label{subsub:archi_tech}
            L'architecture de C-Radar peut être divisée en plusieurs parties (visible en figure \ref{fig:archi}) :
            \begin{itemize}
                \item Différentes bases de données pour différents stockages (une base de données Mongo, une base de données Cassandra et une base de données PostgreSQL) ;
                \item Un moteur de recherche sémantique (Elastic Search) ;
                \item Un gestionnaire de file (RabbitMQ) ;
                \item Différents plugins Python s'interfaçant avec le gestionnaire de file RabbitMQ ;
                \item Un gestionnaire de flux entre les différents composants précédents, le Workflow ;
                \item Une application Java s'interfaçant avec Elastic Search et les bases de données Mongo et PostgreSQL.
            \end{itemize}

            \paragraph{Le JBM ou Java Base Manager :}
                Le JBM est le projet Java qui rassemble le Workflow et l'application \href{app.c-radar.com}{app.c-radar.com}. Ce projet a été conçu et construit au dessus de Spring. Le framework Spring est une plate-forme Java qui fournit une architecture complète permettant de développer des applications Java. Spring gère l'architecture de manière à ce que le développeur n'ait à s'occuper que de l'application. Il prend en charge énormément de tâches dont notamment le modèle MVC, la sécurité de l'application, l'interface avec les gestionnaire de file, etc. (Pour plus d'information, voir \href{http://spring.io/}{http://spring.io/}).

            \paragraph{Le Workflow :}
                Le Workflow est le gestionnaire de flux permettant de lancer les différents processus de récupération et d'analyses des données. Il gère tout ce qui est \og computing \fg. C'est lui qui lance des tâches en les écrivant dans RabbitMQ. Ensuite les plugins Python écoutent RabbitMQ pour obtenir de nouvelles tâches à effectuer. Une fois les sites webs crawlés, le plugin Python va les stocker dans Cassandra sans passer par le Workflow mais en lui indiquant que le crawl est terminé (c'est la seul fois où un plugin Python écrira directement dans une base). Il gère tout ce qui est exécution des plugins Python (crawling et scraping du web, capture et catégorisation des signaux, etc), stockage des données produites à l'issue de ces exécutions et indexation dans Elastic Search.\\
                Pour résumer, il s'occupe de l'orchestration de tous les processus permettant de passer d'un SIREN à une fiche d'entreprise complète, présentable à l'utilisateur via l'application.

            \paragraph{L'application \href{app.c-radar.com}{app.c-radar.com} :}
                L'application, \og présente \fg les données aux utilisateurs. Elle fait le reporting des données produites par le Workflow. Elle permet de rechercher des entreprises, de voir leur répartitions géographiques, de créer des listes d'entreprises, etc. C'est l'application visible et utilisée par l'utilisateur.

            \paragraph{Les bases de données :}
                Les bases de données stockent différents types de données :
                La base MongoDB a pour avantage de pouvoir agréger des documents de sources hétérogènes avec beaucoup de souplesse (base de données NoSQL orientée documents). Elle stocke les données administratives des entreprises, les données issues des sites web, les signaux, ...\\
                RabbitMQ stocke les tâches du Workflow et Cassandra stocke des données de crawl.

            \paragraph{Les plugins Python :}
                Enfin, les plugins Python sont des applications Python isolées, qui exécutent des tâche sur des données. Ils sont isolés, le plus possible, pour éviter les effets de bords. Cela permet de paralléliser au maximum les processus, et, en cas d'erreur dans le traitement, d'être capable de répéter l'opération sans effet sur le reste du système. On utilise le modèle acteur\autocite{modele_acteur} . Ces données sont reçues depuis d'autres composants et le plugin leur retourne les résultats de sa tâche.

            \begin{figure}[h!]
                \centering
                \includegraphics[width=\textwidth]{images/archi.jpg}
                \caption{L'architecture générale de C-Radar}
                \label{fig:archi}
            \end{figure}